
version: '3'

services:
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - ENSURE_NAMENODE_DIR=/hadoop/dfs/name
      - CLUSTER_NAME=hadoop-cluster
    env_file:
      - ./hadoop.env
    command: ["hdfs", "namenode"]
    networks:
      - bigdata_network

  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    restart: always
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    command: ["hdfs", "datanode"]
    networks:
      - bigdata_network
  
  resourcemanager:
    image: apache/hadoop:3.3.6
    container_name: resourcemanager
    hostname: resourcemanager
    restart: always
    depends_on:
      - namenode
    ports:
      - 8088:8088
    environment:
      - SERVICE_PRECONDITION=namenode:9000 namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    command: ["yarn", "resourcemanager"]
    networks:
      - bigdata_network

  nodemanager:
    image: apache/hadoop:3.3.6
    container_name: nodemanager
    restart: always
    depends_on:
      - resourcemanager
    environment:
      - SERVICE_PRECONDITION=resourcemanager:8088
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    command: ["yarn", "nodemanager"]
    networks:
      - bigdata_network

  historyserver:
    image: apache/hadoop:3.3.6
    container_name: historyserver
    restart: always
    depends_on:
      - namenode
      - resourcemanager
    environment:
      - SERVICE_PRECONDITION=resourcemanager:8088
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    command: ["mapred", "historyserver"]
    networks:
      - bigdata_network

  hbase-master:
    image: apache/hbase:2.5.5
    container_name: hbase-master
    hostname: hbase-master
    depends_on:
      - namenode
      - datanode
      - zookeeper
    environment:
      - HBASE_CONF_hbase_rootdir=hdfs://namenode:9000/hbase
      - HBASE_CONF_hbase_cluster_distributed=true
      - HBASE_CONF_hbase_zookeeper_quorum=zookeeper
    ports:
      - 16000:16000
      - 16010:16010
    networks:
      - bigdata_network

  hbase-regionserver:
    image: apache/hbase:2.5.5
    container_name: hbase-regionserver
    hostname: hbase-regionserver
    depends_on:
      - hbase-master
      - zookeeper
    environment:
      - HBASE_CONF_hbase_rootdir=hdfs://namenode:9000/hbase
      - HBASE_CONF_hbase_cluster_distributed=true
      - HBASE_CONF_hbase_zookeeper_quorum=zookeeper
    ports:
      - 16030:16030
    networks:
      - bigdata_network

  metastore-db:
    image: postgres:15.4
    container_name: metastore-db
    environment:
      - POSTGRES_PASSWORD=hive
      - POSTGRES_USER=hive
      - POSTGRES_DB=metastore
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - bigdata_network

  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    hostname: hive-metastore
    depends_on:
      - namenode
      - datanode
      - metastore-db
    environment:
      - DB_DRIVER=postgres
      - DB_CONNECTION_URL=jdbc:postgresql://metastore-db:5432/metastore
      - DB_USER=hive
      - DB_PASS=hive
      - FS_DEFAULT_NAME=hdfs://namenode:9000
    ports:
      - "9083:9083"
    command: /opt/hive/bin/hive --service metastore
    networks:
      - bigdata_network

  hive-server:
    image: apache/hive:3.1.3
    container_name: hive-server
    hostname: hive-server
    depends_on:
      - hive-metastore
    environment:
      - HIVE_METASTORE_URI=thrift://hive-metastore:9083
      - FS_DEFAULT_NAME=hdfs://namenode:9000
    ports:
      - "10000:10000"
      - "10002:10002"
    command: /opt/hive/bin/hive --service hiveserver2
    networks:
      - bigdata_network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.1
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - bigdata_network

  kafka:
    image: confluentinc/cp-kafka:7.5.1
    container_name: kafka
    hostname: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - bigdata_network

  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - bigdata_network

  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    ports:
      - "8081:8081"
    networks:
      - bigdata_network

  jobmanager:
    image: flink:1.18.1-scala_2.12
    container_name: jobmanager
    hostname: jobmanager
    ports:
      - "8083:8081"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 1000
        heartbeat.timeout: 5000
    networks:
      - bigdata_network

  taskmanager:
    image: flink:1.18.1-scala_2.12
    container_name: taskmanager
    depends_on:
      - jobmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 1000
        heartbeat.timeout: 5000
    networks:
      - bigdata_network

networks:
  bigdata_network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  postgres_data:
