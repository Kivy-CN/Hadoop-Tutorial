# Hadoop中的文件存储

## 文件索引节点法详解

索引节点法（inode）是传统文件系统用于管理文件数据存储位置的核心机制。通过类似C语言中的指针概念，文件系统使用索引节点记录文件元数据和数据块的位置信息。

### 索引节点基础结构

```
+---------------------------+
|        索引节点(inode)     |
+---------------------------+
| 文件大小、权限、时间戳等元数据 |
+---------------------------+
| 数据块地址索引项            |
+---------------------------+
```

### 三种地址索引方式

1. **直接地址索引**：索引节点直接存储数据块地址，访问速度最快

```
+---------------+        +------------+
|   索引节点    |        | 数据块(4KB) |
|  iaddr[0~5]  | ------> |            |
+---------------+        +------------+
```

2. **一级间接地址索引**：通过一个间接块指向多个数据块

```
+---------------+     +----------------+     +------------+
|   索引节点    |     |   间接块(4KB)   |     | 数据块(4KB) |
|   iaddr[6]   | --> | 地址1 | 地址2 |..| --> |            |
+---------------+     +----------------+     +------------+
                      1024个数据块地址
```

3. **二级间接地址索引**：通过两级间接块指向更多数据块

```
+---------------+     +----------------+     +----------------+     +------------+
|   索引节点    |     | 一级间接块(4KB) |     | 二级间接块(4KB) |     | 数据块(4KB) |
|   iaddr[7]   | --> | 地址1 | 地址2 |..| --> | 地址1 | 地址2 |..| --> |            |
+---------------+     +----------------+     +----------------+     +------------+
```

### 传统文件系统容量计算

以文档中的例子说明：
- 6个直接地址，每个指向4KB数据块 = 24KB
- 1个一级间接地址，可指向1024个数据块 = 4MB (1024×4KB)
- 1个二级间接地址，可指向1024×1024个数据块 = 4GB (1024×1024×4KB)
- 总文件大小上限 = 4GB + 4MB + 24KB = 4,198,424KB

## Hadoop分布式文件系统中的数据块管理

Hadoop的HDFS是为大规模数据存储设计的分布式文件系统，其数据块处理方式与传统文件系统有显著不同。

### HDFS架构概览

```
+------------------+                      +----------------+
|    NameNode      |                      |   DataNode 1   |
|  (元数据服务器)   |                      |   [Block 1]    |
+------------------+                      |   [Block 3]    |
|  文件名 -> 块列表  |<-------------------->|   [Block 5]    |
|  块ID -> 位置列表 |                      +----------------+
+------------------+                      
        ^                                 +----------------+
        |                                 |   DataNode 2   |
        |                                 |   [Block 1]    |
        |                                 |   [Block 2]    |
        v                                 |   [Block 4]    |
+-----------------+                       +----------------+
|    客户端       |
+-----------------+                       +----------------+
|  读写请求       |                       |   DataNode 3   |
+-----------------+                       |   [Block 2]    |
                                          |   [Block 3]    |
                                          |   [Block 5]    |
                                          +----------------+
```

### HDFS与传统文件系统的区别

| 特性 | 传统文件系统 | HDFS |
|------|------------|------|
| 块大小 | 通常4KB | 默认128MB或256MB |
| 寻址方式 | 多级索引(直接/间接) | 扁平化块列表 |
| 位置管理 | 由文件系统内部处理 | 由NameNode集中管理 |
| 冗余策略 | 通常无内置冗余 | 内置块复制(默认3份) |

### HDFS块大小设计原理

HDFS使用大数据块(128MB默认)而非传统的小块(4KB)，原因在于：

1. **寻址开销减少**：大块减少了元数据管理的开销
2. **网络效率**：大块传输可以最小化网络连接的建立成本
3. **顺序读取优化**：适合数据密集型应用的批量读取

## HDFS容量计算详解

### 1. 单个数据节点容量计算

假设有一个DataNode服务器配置如下：
- 12块4TB硬盘
- HDFS预留10%空间给操作系统和中间数据

```
单个DataNode原始容量 = 12 × 4TB = 48TB
HDFS可用容量 = 48TB × 90% = 43.2TB
```

### 2. 考虑复制因子的实际存储容量

假设复制因子为3(默认值)：

```
有效存储容量 = 总物理容量 ÷ 复制因子
            = 43.2TB ÷ 3 = 14.4TB
```

这意味着虽然集群有43.2TB的物理空间，但因为每个数据块被复制3份，所以只能存储14.4TB的实际数据。

### 3. 集群扩展计算

假设集群有20个DataNode：

```
集群总物理容量 = 单节点容量 × 节点数
                = 43.2TB × 20 = 864TB

集群有效存储容量 = 总物理容量 ÷ 复制因子
                 = 864TB ÷ 3 = 288TB
```

### 4. 数据块数量和NameNode内存需求计算

NameNode需要在内存中维护所有块的元数据。假设：
- 每个数据块在NameNode中占用约150字节的元数据
- 块大小为128MB

```
总数据块数 = 集群有效容量 ÷ 块大小
          = 288TB ÷ 128MB
          = 288 × 1024 × 1024MB ÷ 128MB
          = 288 × 1024 × 8
          = 2,359,296块

NameNode内存需求 = 总块数 × 每块元数据大小
                = 2,359,296 × 150字节
                = 353,894,400字节
                ≈ 337.5MB
```

### 5. 小文件问题及影响

如果存储大量小文件（远小于块大小的文件），每个文件仍占用一个完整的块，会导致：

```
假设平均文件大小为1MB：

实际数据量 = 288TB
理论文件数 = 288TB ÷ 1MB = 301,989,888个文件
实际占用块数 = 301,989,888个块

NameNode内存需求 = 301,989,888 × 150字节 ≈ 43.2GB
```

这是300倍于存储大文件所需的内存需求，说明了小文件对HDFS的巨大影响。

## HDFS与传统索引节点法的对比

```
+------------------+----------------------+------------------------+
| 特性            | 传统inode文件系统      | Hadoop HDFS             |
+------------------+----------------------+------------------------+
| 寻址层次         | 多级索引（直接/间接）  | 扁平化单级块列表         |
| 块大小           | 通常4KB               | 默认128MB               |
| 冗余设计         | 无内置冗余             | 默认3倍数据复制          |
| 元数据管理       | 分散存储于各inode      | 集中存储于NameNode      |
| 最大文件大小     | 受块寻址层级限制       | 基本无限制(PB级)        |
| 优化目标         | 随机访问性能           | 大批量顺序读写          |
+------------------+----------------------+------------------------+
```

## 总结

虽然HDFS和传统文件系统都使用"块"概念来管理数据存储，但它们的实现方式和优化目标有很大差异：

1. 传统文件系统通过复杂的多级索引结构(直接、一级间接、二级间接块)来支持不同大小的文件，优化随机访问性能。

2. HDFS则采用扁平化的大数据块设计，由NameNode集中管理块位置信息，优化大规模数据的连续读写性能。

3. HDFS的容量计算需考虑块大小、复制因子、节点数量和小文件影响，这些因素共同决定了集群的实际可用存储能力和性能特性。

这种设计差异反映了不同系统的应用场景需求：传统文件系统面向通用计算环境，而HDFS专为大数据批处理优化，牺牲了随机访问效率来获得更高的吞吐量和更好的可靠性。